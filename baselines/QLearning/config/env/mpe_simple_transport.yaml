ENV_NAME: MPE_simple_transport
ENV_KWARGS:
  max_steps: 100 # 25

  # reward shaping
  lumber_pickup_reward: 2.5 # reward given to an agent for pickup up lumber if their lumber capacity is > 0
  concrete_pickup_reward: 2.5 # reward given to an agent for pickup of concrete if their capacity is > 0.
  dropoff_reward: 10 # the amount of reward an agent gets for dropping off a resource
  quota_penalty: -0.1

  # capabilities implemented in MPE_simple_transport: [lumber_cap, concrete_cap]
  capability_aware: True # T/F
  num_capabilities: 2
  num_agents: 4

  # NOTE: if these lists are longer than num_agents, env will sample num_agents randomly from them each train iter
  agent_rads: [0.1, 0.1, 0.1, 0.1]
  agent_accels: [2, 2, 2, 2]
  agent_capacities:
    - [[0.9, 0.1], [0.7, 0.3], [1.0, 0.0], [0.0, 1.0]]
    - [[0.9, 0.1], [0.7, 0.3], [0.0, 1.0], [0.2, 0.8]]
    - [[0.8, 0.2], [0.3, 0.7], [0.4, 0.6], [0.7, 0.3]]
    - [[1.0, 0.0], [0.0, 1.0], [0.1, 0.9], [0.3, 0.7]]
    - [[0.6, 0.4], [0.3, 0.7], [0.7, 0.3], [0.0, 1.0]]

  site_quota: [2., 2.] # NOTE overriden by random initialization

  test_team:
    agent_capacities:
      - [[0.2, 0.8], [0.6, 0.4], [0.8, 0.2], [1.0, 0.0]]
      - [[0.0, 1.0], [0.3, 0.7], [0.4, 0.6], [0.7, 0.3]]
      - [[0.0, 1.0], [0.6, 0.4], [0.8, 0.2], [1.0, 0.0]]
      - [[0.2, 0.8], [0.6, 0.4], [0.7, 0.3], [1.0, 0.0]]
      - [[0.3, 0.7], [0.4, 0.6], [0.6, 0.4], [0.9, 0.1]]
      - [[0.1, 0.9], [0.2, 0.8], [0.4, 0.6], [0.9, 0.1]]
      - [[0.1, 0.9], [0.4, 0.6], [0.7, 0.3], [0.9, 0.1]]
      - [[0.1, 0.9], [0.3, 0.7], [0.4, 0.6], [0.8, 0.2]]
      - [[0.1, 0.9], [0.2, 0.8], [0.9, 0.1], [1.0, 0.0]]
      - [[0.1, 0.9], [0.3, 0.7], [0.4, 0.6], [0.7, 0.3]]
  
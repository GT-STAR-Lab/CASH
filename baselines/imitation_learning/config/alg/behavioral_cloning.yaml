
# NOTE: NUM_STEPS (max_steps_per_env) defined by env.yaml
NUM_ENVS: 4

# data gathering
EXPERT_BUFFER_SIZE: 10000 # NOTE: must be divisible by NUM_ENVS
INIT_EXPERT_TRAJ: 10000
DAGGER_ITERATIONS: 1 # 100
DAGGER_TRAJECTORIES_PER_ITER: 0 # 1000
DAGGER_BETA: 1.0 # prob of picking expert actions
# DAGGER_BETA_DECAY: 0.99 # TODO: revert linear decay?
DAGGER_BETA_LINEAR_DECAY: False # True

# update
DAGGER_UPDATES_PER_ITER: 1000
UPDATE_BATCH_SIZE: 64
MAX_GRAD_NORM: 1
LR: 1e-4
LR_LINEAR_DECAY: False
EPS_ADAM: 0.001
WEIGHT_DECAY_ADAM: 0 # 0.001

# eval
NUM_TEST_EPISODES: 32

# agent network params
PARAMETERS_SHARING: True
AGENT_HIDDEN_DIM: 256 # width of the RNN hidden dim + surrounding MLP layers
AGENT_RECURRENT: True
AGENT_INIT_SCALE: 2.
AGENT_HYPERAWARE: False
AGENT_HYPERNET_HIDDEN_DIM: 16 # width of hypernet
AGENT_HYPERNET_INIT_SCALE: 0.2 # NOTE: MUST TUNE THIS

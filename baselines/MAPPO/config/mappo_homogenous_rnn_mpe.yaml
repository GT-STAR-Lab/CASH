LR: 2e-3
ANNEAL_LR: True
NUM_ENVS: 16 
# NUM_STEPS: 128 # defined below by ENV_KWARGS.max_steps
TOTAL_TIMESTEPS: 10e6
FC_DIM_SIZE: 128
GRU_HIDDEN_DIM: 128
UPDATE_EPOCHS: 4
NUM_MINIBATCHES: 4
GAMMA: 0.99
GAE_LAMBDA: 0.95
CLIP_EPS: 0.2
SCALE_CLIP_EPS: False
ENT_COEF: 0.01
VF_COEF: 0.5
MAX_GRAD_NORM: 0.5

AGENT_INIT_SCALE: 2.
AGENT_RECURRENT: True # NOTE: False option not implemented for MAPPO
AGENT_HYPERAWARE: True
AGENT_HYPERNET_KWARGS:
  HIDDEN_DIM: 32 # width of hypernet
  INIT_SCALE: 0.2 # NOTE: MUST TUNE THIS
  USE_LAYER_NORM: True # whether LN is included before ReLU
  NUM_LAYERS: 4 # layers in hypernet, not target net

ENV_NAME: MPE_simple_fire
ENV_KWARGS:
  max_steps: 50 # 25

  fire_rad_range: [0.2, 0.3]

  # reward shaping
  fire_out_reward: 0.5
  uncovered_penalty_factor: 1
  pos_shaping_factor: 0.01

  # capabilities implemented in MPE_simple_fire: [fire_fight_cap, accel]
  capability_aware: True # T/F
  num_capabilities: 2
  num_agents: 3
  num_landmarks: 2

  # NOTE: if these lists are longer than num_agents, env will sample num_agents randomly from them each train iter
  # NOTE: hijacking agent_rads = firefighting capability
  agent_rads: [0.3, 0.2, 0.1, 0.1, 0.2]
  agent_accels: [1, 2, 3, 3, 2]

  test_teams: 
    [[0.09, 3.43, 0.21, 2.94, 0.42, 0.75],
    [0.09, 3.41, 0.21, 3.,   0.48, 0.63],
    [0.05, 3.46, 0.25, 2.76, 0.44, 0.6 ],
    [0.08, 3.23, 0.23, 2.8,  0.5,  0.61],
    [0.09, 3.14, 0.21, 1.16, 0.47, 0.86],
    [0.06, 3.45, 0.24, 2.08, 0.46, 0.76],
    [0.08, 3.06, 0.22, 1.08, 0.48, 0.56],
    [0.07, 3.04, 0.23, 2.37, 0.45, 0.56],
    [0.09, 3.36, 0.21, 2.2,  0.49, 0.64],
    [0.09, 3.26, 0.21, 2.59, 0.47, 0.64]]

SEED: 76
NUM_SEEDS: 3

# WandB Params
ENTITY: star-lab-gt
PROJECT: JaxMARL
WANDB_MODE: online

SAVE_PATH: baselines/MAPPO/checkpoints
VISUALIZE_FINAL_POLICY: True
# number of seeds/envs to visualize
VIZ_NUM_ENVS: 3
